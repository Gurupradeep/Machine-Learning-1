source('C:/Users/Prajwala/Desktop/ex.R')
source('C:/Users/Prajwala/Desktop/ex.R')
source('C:/Users/Prajwala/Desktop/ex.R')
install.packages("xgboost")
library(caret)
install.packages("caret")
library(caret)
install.packages("minqa")
install.packages("pbkrtest")
install.packages("caret")
install.packages("caret")
library(caret)
install.packages("nlopt-devel")
install.packages("caret")
library(caret,"C:/Users/Prajwala/AppData/Local/Temp/RtmpOGSWXx/downloaded_packages")
library(caret,lib.loc= "C:/Users/Prajwala/AppData/Local/Temp/RtmpOGSWXx/downloaded_packages")
dmy <- dummyVars(" ~ .", data = train, fullRank=T)
train <- data.frame(predict(dmy, newdata = train))
library(caret,lib.loc= "C:/Users/Prajwala/AppData/Local/Temp/RtmpOGSWXx/downloaded_packages")
library(xgboost)
library("xgboost", lib.loc="E:/R-3.3.1/library")
detach("package:xgboost", unload=TRUE)
library("xgboost", lib.loc="E:/R-3.3.1/library")
install.packages(xgboost,dependencies=TRUE)
detach("package:xgboost", unload=TRUE)
library("utils", lib.loc="E:/R-3.3.1/library")
library("xgboost", lib.loc="E:/R-3.3.1/library")
detach("package:utils", unload=TRUE)
detach("package:xgboost", unload=TRUE)
library(xgboost)
install.packages("xgboost",dependencies=TRUE)
library(xgboost,lib.loc="C:\sers\Prajwala\AppData\Local\Temp\RtmpKgZOVe\downloaded_packages")
library(xgboost,lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpKgZOVe/downloaded_packages")
library(xgboost,lib.loc="C://Users//Prajwala//AppData//Local//Temp//RtmpKgZOVe//downloaded_packages")
install.packages("xgboost",dependencies=TRUE)
library(xgboost,lib.loc="C://Users//Prajwala//AppData//Local//Temp//RtmpKgZOVe//downloaded_packages")
library(xgboost)#,lib.loc="C://Users//Prajwala//AppData//Local//Temp//RtmpKgZOVe//downloaded_packages")
library(xgboost,lib.loc="C://Users//Prajwala//AppData//Local//Temp//RtmpKgZOVe//downloaded_packages")
library(xgboost,lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpKgZOVe/downloaded_packages")
require(devtools)
install.packages("devtools")
install_github('xgboost','tqchen',subdir='R-package')
install_github('xgboost','tqchen',subdir='R-package')
install.packages("devtools")
library("devtools")
library("devtools")
library("devtools",lib.loc = "C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("devtools",lib.loc = "C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")
library("xgboost", lib.loc="E:/R-3.3.1/library")
detach("package:xgboost", unload=TRUE)
library("tools", lib.loc="E:/R-3.3.1/library")
library("xgboost", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("xgboost", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat_0.0.1", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat_0.0.1.tar.gz", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat_0.1.1.tar.gz", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat_0.1.1", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("drat", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type="source")
.libPaths()
library("xgboost", lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
library("xgboost")#, lib.loc="C:/Users/Prajwala/AppData/Local/Temp/RtmpkLl0bK/downloaded_packages")
installed.packages(lib.loc = ".")
install.packages("xgboost")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("dmlc/xgboost", subdir = "R-package")
devtools::install_github("dmlc/xgboost", subdir = "R-package")
install.packages("ggplot2")
install.packages("Ckmeans.1d.dp")
demo(package = "xgboost")
demo("basic_walkthrough", package = "xgboost")
library(xgboost)
library(xgboost)
library(xgboost)
devtools::install_github("dmlc/xgboost", subdir = "R-package")
devtools::install_github("dmlc/xgboost", subdir = "R-package")
install.packages("xgboost")
library(xgboost)
library(xgboost)
library(xgboost)
library("xgboost", lib.loc="E:/R-3.3.1/library")
install.packages("chron")
library(xgboost)
#Loading libraries
library(xgboost)
library(caret)
library(Metrics)
library(RCurl)
setwd("E:/Computer Engg/Machine learning/RScripts/CoverType")
train = read.csv("train.csv")
test = read.csv("test.csv")
outcome = train[, "Cover_Type"]
outcome<-as.factor(outcome)
levels(outcome)
num.class = length(levels(outcome))
levels(as.factor(outcome)) = 1:num.class
levels(outcome) = 1:num.class
print(levels(outcome))
head(outcome)
train$Cover_Type<-NULL
train$Id<-NULL
cols.without.na = colSums(is.na(test)) == 0
train = train[, cols.without.na]
train[] <- lapply(train, as.numeric)
test[]<-lapply(test, as.numeric)
train.matrix = as.matrix(train)
test.matrix = as.matrix(test)
y = as.matrix(as.integer(outcome)-1)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 16,    # maximum depth of tree
"eta" = 0.01,    # step size shrinkage
"gamma" = 0,    # minimum loss reduction
"subsample" = 1,    # part of data instances to grow tree
"colsample_bytree" = 1,  # subsample ratio of columns when constructing each tree
"min_child_weight" = 12  # minimum sum of instance weight needed in a child
)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 16,    # maximum depth of tree
"eta" = 0.01,    # step size shrinkage
)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 16,    # maximum depth of tree
"eta" = 0.01    # step size shrinkage
)
nround.cv = 200
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
tail(bst.cv$dt)
#Index of minimum merror
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean])
min.merror.idx
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
tail(bst.cv$dt)
#Index of minimum merror
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean])
min.merror.idx
set.seed(1234)
nround.cv = 300
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
tail(bst.cv$dt)
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean])
min.merror.idx
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)
pred.cv = max.col(pred.cv, "last")
confusionMatrix(factor(y+1), factor(pred.cv))
bst <- xgboost(param=param, data=train.matrix, label=y,
nrounds=min.merror.idx, verbose=0)
pred <- predict(bst, test.matrix)
pred = matrix(pred, nrow=num.class, ncol=length(pred)/num.class)
pred = t(pred)
pred = max.col(pred, "last")
print(pred)
model = xgb.dump(bst, with.stats=TRUE)
# get the feature real names
names = dimnames(train.matrix)[[2]]
importance_matrix = xgb.importance(names, model=bst)
test<-read.csv("test.csv")
solution =data.frame(Id = test$Id,Cover_Type = pred)
write.csv(solution,file = "submission.csv",row.names = FALSE)
outcome = train[, "Cover_Type"]
setwd("E:/Computer Engg/Machine learning/RScripts/CoverType")
#Loading the test and the train datasets
train = read.csv("train.csv")
test = read.csv("test.csv")
outcome = train[, "Cover_Type"]
outcome<-as.factor(outcome)
levels(outcome)
num.class = length(levels(outcome))
levels(outcome) = 1:num.class
print(levels(outcome))
head(outcome)
train$Cover_Type<-NULL
train$Id<-NULL
train[] <- lapply(train, as.numeric)
test[]<-lapply(test, as.numeric)
train.matrix = as.matrix(train)
test.matrix = as.matrix(test)
y = as.matrix(as.integer(outcome)-1)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 16,    # maximum depth of tree
"eta" = 0.01 ,   # step size shrinkage,
"gamma" = 0,    # minimum loss reduction
"subsample" = 1,    # part of data instances to grow tree
"colsample_bytree" = 0.35,  # subsample ratio of columns when constructing each tree
"min_child_weight" = 12
)
#Perform n-fold Cross Validation while training
set.seed(1234)
nround.cv = 300
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
tail(bst.cv$dt)
#Index of minimum merror
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean])
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)
pred.cv = max.col(pred.cv, "last")
confusionMatrix(factor(y+1), factor(pred.cv))
trainM<-data.matrix(train, rownames.force = NA);
dtrain <- xgb.DMatrix(data=trainM, label=train.y, missing = NaN);
dtrain <- xgb.DMatrix(data=trainM, label=y, missing = NaN);
watchlist <- list(trainM=dtrain);
clf <- xgb.train(   params              = param,
data                = train.matrix,
nrounds             = min.merror.idx,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = min.merror.idx,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
min.merror.idx
pred <- predict(bst, test.matrix)
pred <- predict(clf, test.matrix)
pred = matrix(pred, nrow=num.class, ncol=length(pred)/num.class)
pred = t(pred)
pred = max.col(pred, "last")
print(pred)
levels(pred)
unique(pred)
test<-read.csv("test.csv")
typeof(test$Id)
solution =data.frame(Id = test$Id,Cover_Type = pred)
write.csv(solution,file = "submission.csv",row.names = FALSE)
#Loading libraries
library(xgboost)
library(caret)
library(Metrics)
library(RCurl)
# Setting up the Working directory
setwd("E:/Computer Engg/Machine learning/RScripts/CoverType")
#Loading the test and the train datasets
train = read.csv("train.csv")
test = read.csv("test.csv")
train$Highwater[train$Vertical_Distance_To_Hydrology < 0]<-0
train$Highwater[train$Vertical_Distance_To_Hydrology > 0]<-1
test$Highwater [test$Vertical_Distance_To_Hydrology < 0]<-0
test$Highwater [test$Vertical_Distance_To_Hydrology > 0]<-1
train$Aspect2[(train$Aspect)+180>360]<-train$Aspect-180
train$Aspect2[(train$Aspect)+180<360]<-train$Aspect+180
outcome = train[, "Cover_Type"]
outcome<-as.factor(outcome)
levels(outcome)
#Convert the character levels to numeric
num.class = length(levels(outcome))
levels(outcome) = 1:num.class
print(levels(outcome))
head(outcome)
#Remove the target column from training set
train$Cover_Type<-NULL
train$Id<-NULL
#If Columns with NA have to be removed
# remove columns with NA, use test data as referal for NA
cols.without.na = colSums(is.na(test)) == 0
train = train[, cols.without.na]
test = test[, cols.without.na]
#Convert REAL to NUMERIC
train[] <- lapply(train, as.numeric)
test[]<-lapply(test, as.numeric)
#Convert data into matrix format compatible for xgboost
train.matrix = as.matrix(train)
test.matrix = as.matrix(test)
# convert outcome from factor to numeric matrix
# xgboost takes multi-labels in [0, numOfClass)
y = as.matrix(as.integer(outcome)-1)
#Set xgboost parameters
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 16,    # maximum depth of tree
"eta" = 0.01 ,   # step size shrinkage,
"gamma" = 0,    # minimum loss reduction
"subsample" = 1,    # part of data instances to grow tree
"colsample_bytree" = 0.35,  # subsample ratio of columns when constructing each tree
"min_child_weight" = 12
)
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 10,    # maximum depth of tree
"eta" = 0.03 ,   # step size shrinkage,
"gamma" = 0,    # minimum loss reduction
"subsample" = 1,    # part of data instances to grow tree
"colsample_bytree" = 0.35,  # subsample ratio of columns when constructing each tree
"min_child_weight" = 12
)
set.seed(1234)
nround.cv = 300
nround.cv = 700
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
cols.without.na = colSums(is.na(test)) == 0
train = train[, cols.without.na]
test = test[, cols.without.na]
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
train$Highwater[train$Vertical_Distance_To_Hydrology <= 0]<-0
train$Highwater[train$Vertical_Distance_To_Hydrology > 0]<-1
test$Highwater [test$Vertical_Distance_To_Hydrology <= 0]<-0
test$Highwater [test$Vertical_Distance_To_Hydrology > 0]<-1
train$Aspect2[(train$Aspect)+180>=360]<-train$Aspect-180
train = read.csv("train.csv")
test = read.csv("test.csv")
train$Highwater[train$Vertical_Distance_To_Hydrology <= 0]<-0
test$Highwater [test$Vertical_Distance_To_Hydrology <= 0]<-0
train$Highwater[train$Vertical_Distance_To_Hydrology > 0]<-1
test$Highwater [test$Vertical_Distance_To_Hydrology > 0]<-1
train$Aspect2[(train$Aspect)+180>=360]<-train$Aspect-180
train$Aspect2[(train$Aspect)+180<360]<-train$Aspect+180
#Extract the target column
outcome = train[, "Cover_Type"]
outcome<-as.factor(outcome)
levels(outcome)
#Convert the character levels to numeric
num.class = length(levels(outcome))
levels(outcome) = 1:num.class
print(levels(outcome))
head(outcome)
#Remove the target column from training set
train$Cover_Type<-NULL
train$Id<-NULL
#If Columns with NA have to be removed
# remove columns with NA, use test data as referal for NA
cols.without.na = colSums(is.na(test)) == 0
train = train[, cols.without.na]
test = test[, cols.without.na]
#Convert REAL to NUMERIC
train[] <- lapply(train, as.numeric)
test[]<-lapply(test, as.numeric)
#Convert data into matrix format compatible for xgboost
train.matrix = as.matrix(train)
test.matrix = as.matrix(test)
# convert outcome from factor to numeric matrix
# xgboost takes multi-labels in [0, numOfClass)
y = as.matrix(as.integer(outcome)-1)
#Set xgboost parameters
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,    # number of classes
"eval_metric" = "merror",    # evaluation metric
"nthread" = 8,   # number of threads to be used
"max_depth" = 10,    # maximum depth of tree
"eta" = 0.03 ,   # step size shrinkage,
"gamma" = 0,    # minimum loss reduction
"subsample" = 1,    # part of data instances to grow tree
"colsample_bytree" = 0.35,  # subsample ratio of columns when constructing each tree
"min_child_weight" = 12
)
#Perform n-fold Cross Validation while training
set.seed(1234)
nround.cv = 700
bst.cv <- xgb.cv(param=param, data=train.matrix, label=y,
nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE)
tail(bst.cv$dt)
#Index of minimum merror
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean])
min.merror.idx
bst.cv[min.merror.idx]
bst.cv$dt[min.merror.idx]
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)
pred.cv = max.col(pred.cv, "last")
confusionMatrix(factor(y+1), factor(pred.cv))
trainM<-data.matrix(train, rownames.force = NA);
dtrain <- xgb.DMatrix(data=trainM, label=y, missing = NaN);
watchlist <- list(trainM=dtrain);
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = min.merror.idx,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
pred <- predict(clf, test.matrix)
#Decode the prediction
pred = matrix(pred, nrow=num.class, ncol=length(pred)/num.class)
pred = t(pred)
pred = max.col(pred, "last")
print(pred)
unique(pred)
##################################
#    Author: Humberto BrandÃ£o    #
##################################
# http://www.humbertobrandao.com #
# http://www.kaggle.com/brandao  #
##################################
cat("Loading libraries...\n");
library(xgboost)
library(data.table)
library(Matrix)
completeData = read.csv("data.csv")
setwd("E:/Computer Engg/Machine learning/RScripts/BasketBallgoal")
cat("Reading CSV file...\n");
completeData = read.csv("data.csv")
cat("Splitting data...\n");
train<-subset(completeData, !is.na(completeData$shot_made_flag));
test<-subset(completeData, is.na(completeData$shot_made_flag));
test.id <- test$shot_id;
train$shot_id <- NULL;
test$shot_id <- NULL;
cat("Creating new features...\n");
train$time_remaining <- train$minutes_remaining*60+train$seconds_remaining;
test$time_remaining <- test$minutes_remaining*60+test$seconds_remaining;
cat("Treating features...\n");
train$shot_distance[train$shot_distance>45] <- 45;
test$shot_distance[test$shot_distance>45] <- 45;
cat("Dropping features...\n");
train$seconds_remaining<-NULL;
test$seconds_remaining<-NULL;
train$team_name <- NULL;
test$team_name <- NULL;
train$team_id <- NULL;
test$team_id <- NULL;
train$game_event_id <- NULL;
train$game_id <- NULL;
test$game_event_id <- NULL;
train$lat <- NULL;
test$game_id <- NULL;
train$lon <- NULL;
test$lat <- NULL;
test$lon <- NULL;
train.y = train$shot_made_flag;
train$shot_made_flag <- NULL;
test$shot_made_flag <- NULL;
pred <- rep(0,nrow(test));
cat("Creating data.matrix...\n");
trainM<-data.matrix(train, rownames.force = NA);
cat("Creating DMarix for xgboost...\n");
dtrain <- xgb.DMatrix(data=trainM, label=train.y, missing = NaN);
watchlist <- list(trainM=dtrain);
set.seed(1984);
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = "logloss",
eta                 = 0.035,
max_depth           = 4,
subsample           = 0.40,
colsample_bytree    = 0.40
)
clf <- xgb.cv(  params              = param,
data                = dtrain,
nrounds             = 1500,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE,
nfold               = 3,
early.stop.round    = 10,
print.every.n       = 1
);
bestRound <- which.min( as.matrix(clf)[,3] );
cat("Best round:", bestRound,"\n");
cat("Best result:",min(as.matrix(clf)[,3]),"\n");
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = bestRound,
verbose             = 1,
watchlist           = watchlist,
maximize            = FALSE
)
testM <-data.matrix(test, rownames.force = NA);
preds <- predict(clf, testM);
submission <- data.frame(shot_id=test.id, shot_made_flag=preds);
cat("Saving the submission file\n");
write.csv(submission, "basicXGBoost.csv", row.names = F);
